{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6789ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in all packages from the jupyter environment\n",
    "import gimmemotifs\n",
    "from gimmemotifs.preprocessing import combine_peaks\n",
    "from gimmemotifs.preprocessing import coverage_table\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import subprocess as sp\n",
    "import qnorm  # either add qnorm to the yml, or run conda install qnorm in the env\n",
    "#import gseapy as gp\n",
    "#from gseapy.plot import barplot, dotplot\n",
    "import itertools\n",
    "import ipywidgets\n",
    "\n",
    "os.chdir(\"/ceph/rimlsfnwi/data/moldevbio/zhou/jsmits/markdown_notebooks/scripts\")\n",
    "from Python_scripts import bedtool_closest\n",
    "from Python_scripts import coverage_table_normalization\n",
    "from Python_scripts import genome_TSS_annotation\n",
    "from Python_scripts import genome_TSS_annotation_prom\n",
    "from Python_scripts import make_autopct\n",
    "from Python_scripts import summits_2_regions\n",
    "from Python_scripts import distance_weight\n",
    "\n",
    "plt.style.use(\"classic\")\n",
    "%matplotlib inline\n",
    "#%load_ext nb_black\n",
    "#%reload_ext nb_black\n",
    "os.chdir(\"/ceph/rimlsfnwi/data/moldevbio/zhou/jsmits/tools/scripts\")\n",
    "from Python_scripts import TSS_to_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7667f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ipython_exit.py\n",
    "Allows exit() to work if script is invoked with IPython without\n",
    "raising NameError Exception. Keeps kernel alive.\n",
    "\n",
    "Use: import variable 'exit' in target script with 'from ipython_exit import exit'    \n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from io import StringIO\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "class IpyExit(SystemExit):\n",
    "    \"\"\"Exit Exception for IPython.\n",
    "\n",
    "    Exception temporarily redirects stderr to buffer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # print(\"exiting\")  # optionally print some message to stdout, too\n",
    "        # ... or do other stuff before exit\n",
    "        sys.stderr = StringIO()\n",
    "\n",
    "    def __del__(self):\n",
    "        sys.stderr.close()\n",
    "        sys.stderr = sys.__stderr__  # restore from backup\n",
    "\n",
    "\n",
    "def ipy_exit():\n",
    "    raise IpyExit\n",
    "\n",
    "\n",
    "if get_ipython():    # ...run with IPython\n",
    "    exit = ipy_exit  # rebind to custom exit\n",
    "else:\n",
    "    exit = exit      # just make exit importable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ddbe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class analysis:\n",
    "    \"\"\"contains all the first steps of my general analysis, including reading a filepath file, config file, and\n",
    "    exporting those and the conda environment to a logdir.\n",
    "    \n",
    "    required input:\n",
    "    datapaths_file, a tsv file containing all the other filepaths needed in the subsequent analysis\n",
    "    config_file, a tsv file containing all type of settings and paramters\n",
    "    output_dir, where to store the output generated by the analysis, it will generate \n",
    "    a log and figure dir in this ouput dir\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datapaths_file: str, config_file: str, output_dir: str, notebook_file: str):\n",
    "        self.datapaths_file = datapaths_file\n",
    "        self.config_file = config_file\n",
    "        self.output_dir = output_dir\n",
    "        self.files = pd.read_table(\n",
    "            self.datapaths_file, sep=\"\\t\", comment=\"#\", index_col=0, header = 0\n",
    "        )\n",
    "        self.config = pd.read_table(\n",
    "            self.config_file, sep=\"\\t\", comment=\"#\", index_col=0, header = 0\n",
    "        )\n",
    "        self.notebook_file = notebook_file\n",
    "        self.logdir = f\"{self.output_dir}/logs\"\n",
    "        self.figdir = f\"{self.output_dir}/figures\"\n",
    "        self.trackhubdir = f\"{self.output_dir}/trackhub\"\n",
    "        self.tmpdir = f\"{self.output_dir}/tmp\"\n",
    "        Path(self.tmpdir).mkdir(parents=True, exist_ok=True)\n",
    "        Path(self.logdir).mkdir(parents=True, exist_ok=True)\n",
    "        Path(self.figdir).mkdir(parents=True, exist_ok=True)\n",
    "        Path(self.trackhubdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    def save_settings(self, conda_path):\n",
    "        \"\"\"save the config and files to the logs, and make a copy of the conda env that his analysis\n",
    "        is ran in. Takes a full path to the conda env location as input\"\"\"\n",
    "        #save the conda environment\n",
    "        !{conda_path} list > {self.logdir}/conda_env.txt\n",
    "        # save both the files and settings of this run to the output_dir\n",
    "        self.files.T.to_csv(f\"{self.logdir}/filepaths.tsv\", header=False, sep=\"\\t\")\n",
    "        self.config.to_csv(f\"{self.logdir}/config.tsv\", header=False, sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03fa2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class general_bedfile_functions:\n",
    "    \"\"\"general functions for working with bedfiles & Chromosome coordinates\"\"\"\n",
    "\n",
    "    def filter_bedfile(\n",
    "        self,\n",
    "        list_chroms_to_remove: list,\n",
    "        output_name: str = None,\n",
    "        bedfile: str = None,\n",
    "        bed_object: str = None,\n",
    "        filter_column: str = \"Chrom\",\n",
    "        file_output=False,\n",
    "        return_output=False,\n",
    "    ):\n",
    "        \"\"\"takes a bedfile with x columns and filter to remove a specific list of strings from a column\n",
    "        if there is no string given, it will try to use the class object df bed.df instead\"\"\"\n",
    "        if bedfile == None:\n",
    "            bed_df = self.bed_df\n",
    "        else:\n",
    "            bed_df = pd.read_table(bedfile, header=None)\n",
    "\n",
    "        bed_df = bed_df[\n",
    "            ~bed_df[filter_column].str.contains(\"|\".join(list_chroms_to_remove))\n",
    "        ]\n",
    "        if file_output == True:\n",
    "            bed_df.to_csv(\n",
    "                f\"{self.output_dir}/{output_name}.bed\",\n",
    "                sep=\"\\t\",\n",
    "                header=False,\n",
    "                index=False,\n",
    "            )\n",
    "        if return_output == True:\n",
    "            return bed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3942cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class identify_prom_windows(analysis, general_bedfile_functions):\n",
    "    \"\"\"identify TSS based promoter windows\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        analysis.__init__(self,*args, **kwargs)\n",
    "\n",
    "    def filter_gtf_file(\n",
    "        self,\n",
    "        gtf_file_name: str = \"genome_gtf\",\n",
    "        filter_column: int = 17,\n",
    "        regex: [str] = '\";protein_coding\"',\n",
    "        skip_lines: int = 5,\n",
    "        rerun: bool = False,\n",
    "    ):\n",
    "        \"\"\"filter a (ensembl) gtf file for only containing data of specific transcripts (e.g. genes)\n",
    "        name of the gtf file in the filepath file\n",
    "        column to use for filtering (default 17, for ensembl protein coding)\n",
    "        regex: regex string to keep.\n",
    "        skip lines: skip the first x lines for skipping meta data of a gtf file\n",
    "        rerun: rerun if file already excists\n",
    "        \"\"\"\n",
    "        # first check if the filtered gtf file already exists\n",
    "        self.filtered_gtf = f\"{self.tmpdir}/filtered_annotation.gtf\"\n",
    "        if not os.path.exists(self.filtered_gtf) or rerun == True:\n",
    "            print(\"filtering GTF_file\")\n",
    "            gtf_file = self.files.loc[gtf_file_name, :].file_location\n",
    "            Path(f\"{self.filtered_gtf}\").touch()\n",
    "            os.remove(f\"{self.filtered_gtf}\")\n",
    "            with open(f\"{self.filtered_gtf}\", \"a\") as output_gtf:\n",
    "                with open(gtf_file) as f:\n",
    "                    for _ in range(skip_lines):\n",
    "                        next(f)\n",
    "                    for full_line in f:\n",
    "                        line = full_line.strip()\n",
    "                        line = line.split()\n",
    "                        if str(line[filter_column]) == regex:\n",
    "                            output_gtf.write(full_line)\n",
    "        else:\n",
    "            print('reusing previously filtered gtf file')\n",
    "        \n",
    "\n",
    "    def make_TSS_window(\n",
    "        self,\n",
    "        TSS_file: str,\n",
    "        sense_antisense_column: int = 6,\n",
    "        gene_name_column: int = 13,\n",
    "        chrom_columnn: int = 0,\n",
    "        chromstart_columnn: int = 3,\n",
    "        chromend_columnn: int = 4,\n",
    "        region_upstream: int = 2000,\n",
    "        region_downstream: int = 2000,\n",
    "        rerun=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes a genome gtf TSS file as input, with column info regarding sense/antisense, chr, chrstart, chrend, and\n",
    "        finally how far up or downstream the window needs to be elongated.\n",
    "        and returns a smaller gtf file containing only the coordinates of the specified TSS region and the gene name.\n",
    "        \"\"\"\n",
    "        # first check if the filtered gtf file already exists\n",
    "        print(\"windowing TSS file\")\n",
    "        tempfile = f\"{self.output_dir}/temp_TSS_window.txt\"\n",
    "        Path(tempfile).touch()\n",
    "        os.remove(f\"{tempfile}\")\n",
    "\n",
    "        with open(f\"{tempfile}\", \"a\") as output_gtf:\n",
    "            with open(TSS_file) as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    line = line.split()\n",
    "                    # calculate the adjusted start and stop coordinates, if smaller than 0, make it 1\n",
    "                    if line[sense_antisense_column] == \"+\":\n",
    "                        chromstart = int(line[chromstart_columnn]) - int(\n",
    "                            region_upstream\n",
    "                        )\n",
    "                        if chromstart <= 0:\n",
    "                            chromstart = 1\n",
    "                        chromend = (int(line[chromstart_columnn]) + 1) + int(\n",
    "                            region_downstream\n",
    "                        )\n",
    "                    elif line[sense_antisense_column] == \"-\":\n",
    "                        chromstart = int(line[chromend_columnn]) - int(\n",
    "                            region_downstream\n",
    "                        )\n",
    "                        if chromstart <= 0:\n",
    "                            chromstart = 1\n",
    "                        chromend = (\n",
    "                            int(line[chromend_columnn]) - 1 + int(region_upstream)\n",
    "                        )\n",
    "                        chromstart_temp = chromstart\n",
    "                        chromstart = chromend\n",
    "                        chromend = chromstart_temp\n",
    "                        \n",
    "\n",
    "                    output_gtf.write(\n",
    "                        str(line[chrom_columnn])\n",
    "                        + \"\\t\"\n",
    "                        + str(chromstart)\n",
    "                        + \"\\t\"\n",
    "                        + str(chromend)\n",
    "                        + \"\\t\"\n",
    "                        + str(line[sense_antisense_column])\n",
    "                        + \"\\t\"\n",
    "                        + str(line[gene_name_column].rstrip(\";\"))\n",
    "                        + \"\\n\"\n",
    "                    )\n",
    "        tempfilesorted = f\"{self.output_dir}/temp_TSS_window_s.txt\"\n",
    "        Path(tempfilesorted).touch()\n",
    "        os.remove(f\"{tempfilesorted}\")\n",
    "\n",
    "        if region_upstream == 0 & region_downstream == 0:\n",
    "            self.bed_df = pd.read_table(tempfile, header=None, names = ['Chrom','ChromStart','ChromEnd','strand','gene'])\n",
    "        else:\n",
    "            sp.check_call(\n",
    "                f\"nice -5 bedtools sort \" f\"-i {tempfile} \" f\"> {tempfilesorted}\",\n",
    "                shell=True,\n",
    "                \n",
    "            )\n",
    "            self.bed_df = pd.read_table(tempfilesorted, header=None, names = ['Chrom','ChromStart','ChromEnd','strand','gene'])\n",
    "            os.remove(tempfilesorted)\n",
    "        os.remove(tempfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8a72662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identify_differential_CREs(identify_prom_windows):\n",
    "    \"\"\"Identify variable CRE elements in my analysis\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        analysis.__init__(self, *args, **kwargs)\n",
    "\n",
    "    def quantify_histon_mod(\n",
    "        self,\n",
    "        regions,\n",
    "        bamfiles,\n",
    "        output_file=None,\n",
    "        alter_window=False,\n",
    "        extend_start=0,\n",
    "        extend_end=0,\n",
    "        rerun=False,\n",
    "        colnames=\"bamfiles\",\n",
    "        coverage_window=200,\n",
    "    ):\n",
    "        \"\"\"quantify the ammount of signal in regions in a bedfile/bedobject within a list of bamfiles.\n",
    "        Optionally you can modify the chromstart and chromend coordinates of the bedfile using alter_window\n",
    "        and giving a extend_start and extend end number in bp\"\"\"\n",
    "        print(\"doing stuff\")\n",
    "        if rerun == True or ((output_file != None) & (not os.path.exists(output_file))):\n",
    "            print(\"quantifying histone mod\")\n",
    "            if alter_window == True:\n",
    "                print(\"resize regions\")\n",
    "                regions[\"ChromStart\"] = regions[\"ChromStart\"].astype(int) - extend_start\n",
    "                regions[\"ChromEnd\"] = regions[\"ChromEnd\"].astype(int) + extend_end\n",
    "\n",
    "            regions.to_csv(\n",
    "                f\"{self.tmpdir}/window_regions_quantwindow.bed\",\n",
    "                sep=\"\\t\",\n",
    "                header=False,\n",
    "                index=False,\n",
    "            )\n",
    "\n",
    "            reads_table = coverage_table(\n",
    "                peakfile=f\"{self.tmpdir}/window_regions_quantwindow.bed\",\n",
    "                datafiles=bamfiles,\n",
    "                window=int(coverage_window),\n",
    "                log_transform=False,\n",
    "                ncpus=int(self.config.loc[\"ncores\", :].vallue),\n",
    "            )\n",
    "            \n",
    "\n",
    "            if colnames != \"bamfiles\":\n",
    "                print(\"replacing column names\")\n",
    "                reads_table.columns = colnames\n",
    "                \n",
    "            reads_table = reads_table.drop_duplicates()\n",
    "            reads_table = reads_table.astype(int)\n",
    "            reads_table.insert(loc=0, column='region', value=reads_table.index)\n",
    "\n",
    "            if output_file != None:\n",
    "                reads_table.to_csv(\n",
    "                    f\"{output_file}\",\n",
    "                    sep=\"\\t\",\n",
    "                    header=True,\n",
    "                    index=False,\n",
    "                )\n",
    "            return reads_table\n",
    "        \n",
    "        if rerun == False and ((output_file != None) & (os.path.exists(output_file))):\n",
    "            print('using previously generated table')\n",
    "            reads_table = pd.read_table(f\"{output_file}\",sep=\"\\t\",index_col = 0)        \n",
    "            return reads_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b304093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_CREs = Identify_differential_CREs(\n",
    "    #datapaths_file=\"/ceph/rimlsfnwi/data/moldevbio/zhou/jarts/data/lako2021/merge_peaks/files3.tsv\",\n",
    "    datapaths_file=\"/ceph/rimlsfnwi/data/moldevbio/zhou/jarts/data/scANANSE_11032022/ATAC_qquant/files_meta.tsv\",\n",
    "    config_file=\"/ceph/rimlsfnwi/data/moldevbio/zhou/jarts/data/scANANSE_11032022/ATAC_qquant/config.tsv\",\n",
    "    output_dir=\"/ceph/rimlsfnwi/data/moldevbio/zhou/jarts/data/scANANSE/ATAC_qquant/\",\n",
    "    notebook_file=\"/ceph/rimlsfnwi/data/moldevbio/zhou/jarts/jupyter_notebooks/Combine_peaks.ipynb\",\n",
    ")\n",
    "\n",
    "ID_CREs.main_dir = (\n",
    "    \"/ceph/rimlsfnwi/data/moldevbio/zhou/jarts/data/scANANSE/ATAC_qquant/\"\n",
    ")\n",
    "# datapaths_file=\"/ceph/rimlsfnwi/data/moldevbio/zhou/jarts/data/lako2021/merge_peaks/files.tsv\" for all 11 populations\n",
    "# datapaths_file=\"/ceph/rimlsfnwi/data/moldevbio/zhou/jarts/data/lako2021/merge_peaks/files2.tsv\" for the \"four\" cell populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27953359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 14:26:54,086 - INFO - Loading data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Merging peaks with the cell types: CE'\n",
      " 'Merging peaks with the cell types: CF'\n",
      " 'Merging peaks with the cell types: CSSC'\n",
      " 'Merging peaks with the cell types: Cj'\n",
      " 'Merging peaks with the cell types: LE'\n",
      " 'Merging peaks with the cell types: LESC'\n",
      " 'Merging peaks with the cell types: LSC']\n",
      "CE\n",
      "filename\n",
      "CE_ATAC_BAM    Generating the coverage table for cell populat...\n",
      "Name: file_location, dtype: object\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39cbcc34810f4df0b07e525a168c76f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-5:\n",
      "  File \"/vol/mbconda/julian/envs/jupyter/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/vol/mbconda/julian/envs/jupyter/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/vol/mbconda/julian/envs/jupyter/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/vol/mbconda/julian/envs/jupyter/lib/python3.8/site-packages/fluff/fluffio.py\", line 185, in load_heatmap_data\n",
      "    result = track.binned_stats(tmp.name, bins, split=True, rpkm=rpkm)\n",
      "  File \"/vol/mbconda/julian/envs/jupyter/lib/python3.8/site-packages/fluff/track.py\", line 189, in binned_stats\n",
      "    for feature, min_strand, plus_strand in self.fetch_to_counts(in_track):\n",
      "  File \"/vol/mbconda/julian/envs/jupyter/lib/python3.8/site-packages/fluff/track.py\", line 331, in fetch_to_counts\n",
      "    for read in self.track.fetch(feature.chrom, feature.start, feature.end):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/mbconda/julian/envs/jupyter/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/vol/mbconda/julian/envs/jupyter/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/vol/mbconda/julian/envs/jupyter/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/vol/mbconda/julian/envs/jupyter/lib/python3.8/multiprocessing/queues.py\", line 356, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/vol/mbconda/julian/envs/jupyter/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/vol/mbconda/julian/envs/jupyter/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/vol/mbconda/julian/envs/jupyter/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "#Merging peaks and generating individual count files\n",
    "merged_summits = list()\n",
    "if not os.path.exists(f\"{ID_CREs.output_dir}/accesible_summits_reps.bed\"):\n",
    "    ATAC_BAM = ID_CREs.files[ID_CREs.files.data_type == \"ATAC_BAM\"]\n",
    "    ATAC_BAM = ATAC_BAM.file_location\n",
    "    ATAC_peaks = ID_CREs.files[ID_CREs.files.data_type == \"ATAC_peak\"]\n",
    "    ATAC_pfiles = ATAC_peaks.file_location \n",
    "    combined_ATAC_peaks = combine_peaks(\n",
    "        list(ATAC_pfiles),\n",
    "        #print(list(ATAC_pfiles)),\n",
    "        ID_CREs.files.loc[\"genome_path_size\", :].file_location,\n",
    "        ID_CREs.config.loc[\"ATAC_summit\", :].vallue,\n",
    "        True,\n",
    "    )\n",
    "    combined_ATAC_peaks.to_csv(\n",
    "        f\"{ID_CREs.tmpdir}/all_merged_ATAC_peaks.bed\",\n",
    "        sep=\"\\t\",\n",
    "        header=False,\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    # quantify the peak intensity per cell type\n",
    "    cell_types = np.unique(\n",
    "        ID_CREs.files[ID_CREs.files.data_type == \"ATAC_BAM\"].cell_type,\n",
    "        return_index=False,\n",
    "    )\n",
    "    print(\"Merging peaks with the cell types: \" + cell_types)\n",
    "    for cell in cell_types:\n",
    "        print(cell)\n",
    "        ATAC_BAM = ID_CREs.files[ID_CREs.files.data_type == \"ATAC_BAM\"]\n",
    "        ATAC_BAM = ATAC_BAM[ATAC_BAM.cell_type == cell].file_location\n",
    "        print(\"Generating the coverage table for cell population \" + ATAC_BAM)\n",
    "        peak_counts = coverage_table(\n",
    "             peakfile=f\"{ID_CREs.tmpdir}/all_merged_ATAC_peaks.bed\",\n",
    "             datafiles=list(ATAC_BAM),\n",
    "             window=int(ID_CREs.config.loc[\"ATAC_summit\", :][0]),\n",
    "             log_transform=False,\n",
    "             ncpus=int(ID_CREs.config.loc[\"ncores\", :][0]),\n",
    "         )\n",
    "        peak_counts = peak_counts[\n",
    "             ~peak_counts.index.str.contains(\n",
    "                 \"|\".join([\"GL\", \"Un\", \"KI\", \"MT\", \"X\", \"Y\"])\n",
    "             )\n",
    "         ]\n",
    "        final_df = peak_counts\n",
    "        final_df.index.name = \"loc\"\n",
    "        final_df.to_csv(\n",
    "            f\"{ID_CREs.tmpdir}/{str(cell)}_covtable.tsv\",\n",
    "            sep=\"\\t\",\n",
    "            header=True,\n",
    "            index=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "592bdded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining the individual coverage tables to one\n"
     ]
    }
   ],
   "source": [
    "# Concatenating all cell files to one final file\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "path = f\"{ID_CREs.tmpdir}/\"\n",
    "all_files = []\n",
    "\n",
    "cell_types = np.unique(\n",
    "        ID_CREs.files[ID_CREs.files.data_type == \"ATAC_BAM\"].cell_type,\n",
    "        return_index=False,\n",
    "    )\n",
    "\n",
    "# Joining all cov table locations into one list\n",
    "for cell in cell_types:\n",
    "    all_files = glob.glob(path + f\"*covtable.tsv\")\n",
    "\n",
    "print(\"Joining the individual coverage tables to one\")\n",
    "\n",
    "if not os.path.exists(f\"{ID_CREs.tmpdir}/joinedcovtable.tsv\"):\n",
    "\n",
    "    # Joining all files to one dataframe\n",
    "    dffull = pd.concat((pd.read_table(f) for f in all_files),axis=1)\n",
    "\n",
    "    # Merging the loc columns to one\n",
    "    dffull = dffull.groupby(level=0, axis=1).first()\n",
    "\n",
    "    # Setting loc as the index\n",
    "    dffull.index = dffull['loc']\n",
    "    del dffull['loc']\n",
    "\n",
    "    dffull.to_csv(\n",
    "                f\"{ID_CREs.tmpdir}/joinedcovtable.tsv\",\n",
    "                sep=\"\\t\",\n",
    "                header=True,\n",
    "                index=True,\n",
    "            )\n",
    "\n",
    "    dffull\n",
    "    # Remove all regions that have a sum of zero counts\n",
    "    dffull2 = dffull.loc[~(dffull==0).all(axis=1)]\n",
    "    dffull2.to_csv(\n",
    "                f\"{ID_CREs.tmpdir}/joinedcovtable_no_zero.tsv\",\n",
    "                sep=\"\\t\",\n",
    "                header=True,\n",
    "                index=True,\n",
    "            )\n",
    "    dffull2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21457d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
